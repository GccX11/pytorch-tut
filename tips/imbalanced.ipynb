{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "# Methods for dealing with imbalanced datasets:\n",
    "# 1. Over-sampling: Add more copies of the minority class\n",
    "# 2. Class weighting: Increase the loss for the minority class\n",
    "\n",
    "# Over-sampling\n",
    "# 1. Random over-sampling: Add random copies of the minority class\n",
    "# 2. Synthetic over-sampling: Generate synthetic samples of the minority class using techniques like SMOTE\n",
    "\n",
    "# Class weighting\n",
    "# 1. Assign a weight to each class in the loss function\n",
    "# 2. The weight of the minority class is higher than the weight of the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Class weighting\n",
    "# Weight of the minority class is 50\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1, 50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Random over-sampling (using WeightedRandomSampler)\n",
    "def get_loader(root_dir, batch_size):\n",
    "    my_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.ImageFolder(root=root_dir, transform=my_transforms)\n",
    "    \n",
    "    # get the inverse of the number of samples in each class\n",
    "    # and use them as class weights\n",
    "    class_weights = []\n",
    "    for root, subdir, files in os.walk(root_dir):\n",
    "        if len(files) > 0: # skip empty folders\n",
    "            class_weights.append(1/len(files))\n",
    "\n",
    "    sample_weights = [0] * len(dataset)\n",
    "    # create the sample_weights\n",
    "    # (weight for each sample based on its class)\n",
    "    for idx, (data, label) in enumerate(dataset):\n",
    "        class_weight = class_weights[label]\n",
    "        sample_weights[idx] = class_weight\n",
    "\n",
    "    # we can use a WeightedRandomSampler to oversample the minority class\n",
    "    # replacement=True to make sure we can see examples more than once\n",
    "    # otherwise it won't over-sample\n",
    "    sampler = WeightedRandomSampler(sample_weights, \n",
    "                                    num_samples=len(sample_weights), \n",
    "                                    replacement=True)\n",
    "    \n",
    "    # make our loader and specify the sampler\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BufferedReader name='dataset/PetImages/Cat/666.jpg'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader \u001b[38;5;241m=\u001b[39m get_loader(root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/PetImages\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(label)\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mget_loader\u001b[0;34m(root_dir, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# create the sample_weights\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# (weight for each sample based on its class)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (data, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m     21\u001b[0m     class_weight \u001b[38;5;241m=\u001b[39m class_weights[label]\n\u001b[1;32m     22\u001b[0m     sample_weights[idx] \u001b[38;5;241m=\u001b[39m class_weight\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 247\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.11/site-packages/PIL/Image.py:3283\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3281\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3282\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3283\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BufferedReader name='dataset/PetImages/Cat/666.jpg'>"
     ]
    }
   ],
   "source": [
    "loader = get_loader(root_dir='dataset/PetImages', batch_size=8)\n",
    "for data, label in loader:\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
